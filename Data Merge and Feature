{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "679f0e7e-7720-438a-8087-b86d98b6ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62fb4004-2532-4747-a9ff-3529e19566d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_data_path = r\"C:\\Users\\DELL\\Downloads\\transactions_data.csv\"\n",
    "customers_data_path = r\"C:\\Users\\DELL\\Downloads\\customers_data.csv\"\n",
    "products_data_path = r\"C:\\Users\\DELL\\Downloads\\products_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e45c7007-be6c-446e-ab1a-868adbaca366",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_data = pd.read_csv(transactions_data_path)\n",
    "customers_data = pd.read_csv(customers_data_path)\n",
    "products_data = pd.read_csv(products_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58ddc8d4-4983-45d7-84c1-9d933e6cbb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Transaction_ID  Company_ID  Product_ID  Quantity  \\\n",
      "0         0.0             1.0        88.0         6.0       NaN   \n",
      "1         1.0             2.0        29.0        19.0      16.0   \n",
      "2         2.0             NaN        28.0        18.0       6.0   \n",
      "3         3.0             4.0        85.0        12.0      12.0   \n",
      "4         4.0             5.0        47.0         3.0       8.0   \n",
      "\n",
      "  Transaction_Date  Product_Price  Total_Cost  \n",
      "0       2024/03/26  194379.147964   1075200.0  \n",
      "1    July 09, 2024   97930.993380   1428000.0  \n",
      "2       04/13/2024  126095.547778    940800.0  \n",
      "3       09-06-2023            NaN   1008000.0  \n",
      "4       07/06/2021   99575.609634    705600.0  \n",
      "   Company_ID          Company_Name  Company_Profit  \\\n",
      "0         1.0  Tech  Enterprises  1         80701.0   \n",
      "1         2.0   Global  Partners  2         80511.0   \n",
      "2         3.0  Quantum Associates 3        110664.0   \n",
      "3         4.0       Prime Network 4             NaN   \n",
      "4         5.0    Elite  Ventures  5         69427.0   \n",
      "\n",
      "                                             Address  \n",
      "0             EDSA, Barangay 606, Pasig, Philippines  \n",
      "1  Commonwealth Ave, Barangay 789, Taguig, Philip...  \n",
      "2       Roxas Blvd, Barangay 505, Pasig, Philippines  \n",
      "3  Alabang-Zapote Rd, Barangay 202, Taguig, Phili...  \n",
      "4    Ayala Avenue, Barangay 101, Makati, Philippines  \n",
      "   Product_ID            Product_Name Product_Price\n",
      "0         1.0      FinPredictor Suite      ?140,000\n",
      "1         2.0  MarketMinder Analytics      ?168,000\n",
      "2         3.0    TrendWise Forecaster      ?100,800\n",
      "3         4.0  CustomerScope Insights      ?123,200\n",
      "4         5.0     SalesSync Optimizer       ?84,000\n"
     ]
    }
   ],
   "source": [
    "print(transactions_data.head())\n",
    "print(customers_data.head())\n",
    "print(products_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f3e325f-72c0-46af-a57e-ff81b263a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_data = transactions_data.dropna(subset=['Transaction_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08253a99-b3ec-45c4-89e7-2784c04bee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_data['Transaction_Date'] = pd.to_datetime(transactions_data['Transaction_Date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0818b027-d3b8-4f5c-8dd8-61b4fd441608",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_data['Product_Price'] = products_data['Product_Price'].replace({'\\\\?': '', ',': ''}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f55a74c6-99de-4598-a3fd-bf591b695227",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_data['Product_Price'] = pd.to_numeric(products_data['Product_Price'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "974cdf6b-1e42-4783-8071-c32e40e60588",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_data['Company_Profit'] = customers_data['Company_Profit'].fillna(customers_data['Company_Profit'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1be9b57d-0746-4b7d-ad60-e114dedda8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions Data After Cleaning:\n",
      "   Unnamed: 0  Transaction_ID  Company_ID  Product_ID  Quantity  \\\n",
      "0         0.0             1.0        88.0         6.0       NaN   \n",
      "1         1.0             2.0        29.0        19.0      16.0   \n",
      "3         3.0             4.0        85.0        12.0      12.0   \n",
      "4         4.0             5.0        47.0         3.0       8.0   \n",
      "5         5.0             6.0        80.0        11.0       4.0   \n",
      "\n",
      "  Transaction_Date  Product_Price  Total_Cost  \n",
      "0       2024-03-26  194379.147964   1075200.0  \n",
      "1              NaT   97930.993380   1428000.0  \n",
      "3              NaT            NaN   1008000.0  \n",
      "4              NaT   99575.609634    705600.0  \n",
      "5       2021-07-12  160658.675350    627200.0  \n",
      "\n",
      "Products Data After Cleaning:\n",
      "   Product_ID            Product_Name  Product_Price\n",
      "0         1.0      FinPredictor Suite         140000\n",
      "1         2.0  MarketMinder Analytics         168000\n",
      "2         3.0    TrendWise Forecaster         100800\n",
      "3         4.0  CustomerScope Insights         123200\n",
      "4         5.0     SalesSync Optimizer          84000\n",
      "\n",
      "Customers Data After Cleaning:\n",
      "   Company_ID          Company_Name  Company_Profit  \\\n",
      "0         1.0  Tech  Enterprises  1         80701.0   \n",
      "1         2.0   Global  Partners  2         80511.0   \n",
      "2         3.0  Quantum Associates 3        110664.0   \n",
      "3         4.0       Prime Network 4         75301.5   \n",
      "4         5.0    Elite  Ventures  5         69427.0   \n",
      "\n",
      "                                             Address  \n",
      "0             EDSA, Barangay 606, Pasig, Philippines  \n",
      "1  Commonwealth Ave, Barangay 789, Taguig, Philip...  \n",
      "2       Roxas Blvd, Barangay 505, Pasig, Philippines  \n",
      "3  Alabang-Zapote Rd, Barangay 202, Taguig, Phili...  \n",
      "4    Ayala Avenue, Barangay 101, Makati, Philippines  \n"
     ]
    }
   ],
   "source": [
    "print(\"Transactions Data After Cleaning:\")\n",
    "print(transactions_data.head())\n",
    "\n",
    "print(\"\\nProducts Data After Cleaning:\")\n",
    "print(products_data.head())\n",
    "\n",
    "print(\"\\nCustomers Data After Cleaning:\")\n",
    "print(customers_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97380089-785c-4f41-ba01-ab85c4bff95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(transactions_data, customers_data, on='Company_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "424fbc68-bdce-45a9-81d5-9da286a9c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(merged_data, products_data, on='Product_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb0391ec-8da9-45c5-90ea-67c9d373a2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Transaction_ID  Company_ID  Product_ID  Quantity  \\\n",
      "0         0.0             1.0        88.0         6.0       NaN   \n",
      "1         1.0             2.0        29.0        19.0      16.0   \n",
      "2         3.0             4.0        85.0        12.0      12.0   \n",
      "3         4.0             5.0        47.0         3.0       8.0   \n",
      "4         5.0             6.0        80.0        11.0       4.0   \n",
      "\n",
      "  Transaction_Date  Product_Price_x  Total_Cost           Company_Name  \\\n",
      "0       2024-03-26    194379.147964   1075200.0    Elite Consulting 88   \n",
      "1              NaT     97930.993380   1428000.0    Sky  Industries  29   \n",
      "2              NaT              NaN   1008000.0      Green Ventures 85   \n",
      "3              NaT     99575.609634    705600.0  Green  Industries  47   \n",
      "4       2021-07-12    160658.675350    627200.0    Green  Partners  80   \n",
      "\n",
      "   Company_Profit                                            Address  \\\n",
      "0         75950.0            EDSA, Barangay 456, Taguig, Philippines   \n",
      "1         61952.0              Edsa, brgy. 606, makati, philippines!   \n",
      "2        113470.0         EDSA, Barangay 707, Cebu City, Philippines   \n",
      "3         31130.0   Taft Ave, Barangay 707, Mandaluyong, Philippines   \n",
      "4        111227.0  Commonwealth Ave, Barangay 202, Manila, Philip...   \n",
      "\n",
      "           Product_Name  Product_Price_y  \n",
      "0  RevenueVue Dashboard         179200.0  \n",
      "1      EcoNomix Modeler          95200.0  \n",
      "2      BudgetMaster Pro          84000.0  \n",
      "3  TrendWise Forecaster         100800.0  \n",
      "4   OptiFlow Automation         156800.0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18747 entries, 0 to 18746\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   Unnamed: 0        16817 non-null  float64       \n",
      " 1   Transaction_ID    18747 non-null  float64       \n",
      " 2   Company_ID        8917 non-null   float64       \n",
      " 3   Product_ID        15507 non-null  float64       \n",
      " 4   Quantity          16946 non-null  float64       \n",
      " 5   Transaction_Date  4824 non-null   datetime64[ns]\n",
      " 6   Product_Price_x   16902 non-null  float64       \n",
      " 7   Total_Cost        16973 non-null  float64       \n",
      " 8   Company_Name      17858 non-null  object        \n",
      " 9   Company_Profit    17858 non-null  float64       \n",
      " 10  Address           17858 non-null  object        \n",
      " 11  Product_Name      17271 non-null  object        \n",
      " 12  Product_Price_y   17271 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(9), object(3)\n",
      "memory usage: 1.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(merged_data.head())\n",
    "print(merged_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d74ac74-2e73-4719-bb0d-b7cfd4e5ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8d58c30-4b75-4a8a-be05-5745eb44a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.dropna(subset=['Transaction_ID', 'Product_ID', 'Company_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d2b71bcc-43d5-423a-90c5-8465f833855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['Transaction_Date'] = merged_data['Transaction_Date'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1d99d93-7fe1-4138-b527-8b0e242a691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['Product_Price'] = merged_data['Product_Price_y']\n",
    "merged_data = merged_data.drop(columns=['Product_Price_x', 'Product_Price_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c06f9fc4-b3a4-4eb5-879a-cf6a5cbe8af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction_ID        0\n",
      "Company_ID            0\n",
      "Product_ID            0\n",
      "Quantity            739\n",
      "Transaction_Date      0\n",
      "Total_Cost          712\n",
      "Company_Name        721\n",
      "Company_Profit      721\n",
      "Address             721\n",
      "Product_Name        726\n",
      "Product_Price       726\n",
      "dtype: int64\n",
      "   Transaction_ID  Company_ID  Product_ID  Quantity     Transaction_Date  \\\n",
      "0             1.0        88.0         6.0       NaN  2024-03-26 00:00:00   \n",
      "1             2.0        29.0        19.0      16.0              Unknown   \n",
      "2             4.0        85.0        12.0      12.0              Unknown   \n",
      "3             5.0        47.0         3.0       8.0              Unknown   \n",
      "4             6.0        80.0        11.0       4.0  2021-07-12 00:00:00   \n",
      "\n",
      "   Total_Cost           Company_Name  Company_Profit  \\\n",
      "0   1075200.0    Elite Consulting 88         75950.0   \n",
      "1   1428000.0    Sky  Industries  29         61952.0   \n",
      "2   1008000.0      Green Ventures 85        113470.0   \n",
      "3    705600.0  Green  Industries  47         31130.0   \n",
      "4    627200.0    Green  Partners  80        111227.0   \n",
      "\n",
      "                                             Address          Product_Name  \\\n",
      "0            EDSA, Barangay 456, Taguig, Philippines  RevenueVue Dashboard   \n",
      "1              Edsa, brgy. 606, makati, philippines!      EcoNomix Modeler   \n",
      "2         EDSA, Barangay 707, Cebu City, Philippines      BudgetMaster Pro   \n",
      "3   Taft Ave, Barangay 707, Mandaluyong, Philippines  TrendWise Forecaster   \n",
      "4  Commonwealth Ave, Barangay 202, Manila, Philip...   OptiFlow Automation   \n",
      "\n",
      "   Product_Price  \n",
      "0       179200.0  \n",
      "1        95200.0  \n",
      "2        84000.0  \n",
      "3       100800.0  \n",
      "4       156800.0  \n"
     ]
    }
   ],
   "source": [
    "print(merged_data.isnull().sum())\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "532a5cea-eebe-4e5c-be83-d8a1c7809aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['Quantity'] = merged_data['Quantity'].fillna(merged_data['Quantity'].median())\n",
    "merged_data['Total_Cost'] = merged_data['Total_Cost'].fillna(merged_data['Total_Cost'].median())\n",
    "merged_data['Product_Price'] = merged_data['Product_Price'].fillna(merged_data['Product_Price'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "951219d3-1623-41aa-9010-944ac9e32bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['Company_Name'] = merged_data['Company_Name'].fillna('Unknown')\n",
    "merged_data['Company_Profit'] = merged_data['Company_Profit'].fillna(merged_data['Company_Profit'].median())  # Filling with median for consistency\n",
    "merged_data['Address'] = merged_data['Address'].fillna('Unknown')\n",
    "merged_data['Product_Name'] = merged_data['Product_Name'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "84abc55f-2d77-43f2-8301-c3d8bc931b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction_ID      0\n",
      "Company_ID          0\n",
      "Product_ID          0\n",
      "Quantity            0\n",
      "Transaction_Date    0\n",
      "Total_Cost          0\n",
      "Company_Name        0\n",
      "Company_Profit      0\n",
      "Address             0\n",
      "Product_Name        0\n",
      "Product_Price       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(merged_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec5a6cdd-2dac-4002-8b1a-28d820901c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['Transaction_Date'] = pd.to_datetime(merged_data['Transaction_Date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ef5e1856-074e-4c2d-ad83-52522e42f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['Recency'] = (merged_data['Transaction_Date'].max() - merged_data['Transaction_Date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "713a4fc9-a234-4134-ab0e-958d52baac1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Company_ID Transaction_Date  Recency\n",
      "0        88.0       2024-03-26    216.0\n",
      "1        29.0              NaT      NaN\n",
      "2        85.0              NaT      NaN\n",
      "3        47.0              NaT      NaN\n",
      "4        80.0       2021-07-12   1204.0\n"
     ]
    }
   ],
   "source": [
    "print(merged_data[['Company_ID', 'Transaction_Date', 'Recency']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "795860b7-50f1-4a61-b3ed-4aab44536134",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_date = merged_data['Transaction_Date'].median()\n",
    "merged_data['Transaction_Date'] = merged_data['Transaction_Date'].fillna(median_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "60b6644a-1e8d-4c37-b4de-9a4cc218fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['Recency'] = (merged_data['Transaction_Date'].max() - merged_data['Transaction_Date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "31e3d60b-c225-4b16-96bd-dcfd1366b03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Company_ID    Transaction_Date  Recency\n",
      "0        88.0 2024-03-26 00:00:00      216\n",
      "1        29.0 2022-10-15 12:00:00      743\n",
      "2        85.0 2022-10-15 12:00:00      743\n",
      "3        47.0 2022-10-15 12:00:00      743\n",
      "4        80.0 2021-07-12 00:00:00     1204\n"
     ]
    }
   ],
   "source": [
    "print(merged_data[['Company_ID', 'Transaction_Date', 'Recency']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d8b01cc9-cb4a-46e5-8f16-2d9283bcd651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f354abec-a96c-43f9-b8f6-8e24c430c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['Purchase_Likelihood'] = (merged_data['Quantity'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "db62a612-f656-48dc-91a4-67dcf1c5f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Product_Price', 'Recency', 'Purchase_Frequency', 'Total_Spending']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "770c0300-8dbc-4af7-be1d-e20ea262358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Purchase_Likelihood'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "af87798d-de3f-468b-9a5c-21559d68bdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Transaction_ID', 'Company_ID', 'Product_ID', 'Quantity',\n",
      "       'Transaction_Date', 'Total_Cost', 'Company_Name', 'Company_Profit',\n",
      "       'Address', 'Product_Name', 'Product_Price', 'Recency',\n",
      "       'Purchase_Likelihood'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(merged_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0b86f464-ab78-4b0e-9737-4d8a9c1457d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_frequency = merged_data.groupby('Company_ID')['Transaction_ID'].count().reset_index()\n",
    "purchase_frequency.columns = ['Company_ID', 'Purchase_Frequency']\n",
    "merged_data = merged_data.merge(purchase_frequency, on='Company_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8d98f7ae-8372-4196-be77-58287c8b5e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_spending = merged_data.groupby('Company_ID')['Total_Cost'].sum().reset_index()\n",
    "total_spending.columns = ['Company_ID', 'Total_Spending']\n",
    "merged_data = merged_data.merge(total_spending, on='Company_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c9d119dd-c65f-49b0-948c-98339b5bd40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Company_ID  Purchase_Frequency  Total_Spending\n",
      "0        88.0                  70     102670400.0\n",
      "1        29.0                  58      91252000.0\n",
      "2        85.0                  82     115130400.0\n",
      "3        47.0                  48      55731200.0\n",
      "4        80.0                  83     125462400.0\n"
     ]
    }
   ],
   "source": [
    "print(merged_data[['Company_ID', 'Purchase_Frequency', 'Total_Spending']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f80e6478-b2d4-452b-96b5-06c7270e1acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9862637362637363\n",
      "Confusion Matrix:\n",
      " [[   0   19]\n",
      " [   1 1436]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        19\n",
      "           1       0.99      1.00      0.99      1437\n",
      "\n",
      "    accuracy                           0.99      1456\n",
      "   macro avg       0.49      0.50      0.50      1456\n",
      "weighted avg       0.97      0.99      0.98      1456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Step 1: Define the target variable and features\n",
    "# Create a binary target variable ('1' for purchase, '0' for no purchase)\n",
    "merged_data['Purchase_Likelihood'] = (merged_data['Quantity'] > 0).astype(int)\n",
    "\n",
    "# Features: selecting relevant columns\n",
    "features = ['Product_Price', 'Recency', 'Purchase_Frequency', 'Total_Spending']\n",
    "\n",
    "# Target: the new binary variable\n",
    "target = 'Purchase_Likelihood'\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X = merged_data[features]  # Features\n",
    "y = merged_data[target]  # Target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Select a machine learning algorithm (Random Forest Classifier)\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Step 4: Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ece45190-c658-4b13-bfa4-afdb5e95452f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9587912087912088\n",
      "Confusion Matrix:\n",
      " [[   2   17]\n",
      " [  43 1394]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.04      0.11      0.06        19\n",
      "           1       0.99      0.97      0.98      1437\n",
      "\n",
      "    accuracy                           0.96      1456\n",
      "   macro avg       0.52      0.54      0.52      1456\n",
      "weighted avg       0.98      0.96      0.97      1456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply class weights to handle class imbalance\n",
    "model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Train the model with class weights\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate again\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2298b64-b61d-4bcb-a776-53bcafe3766d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, confusion_matrix, classification_report\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Prepare data\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m merged_data[features]  \u001b[38;5;66;03m# Features\u001b[39;00m\n\u001b[0;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m merged_data[target]  \u001b[38;5;66;03m# Target\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Split data into train and test\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_data' is not defined"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Prepare data\n",
    "X = merged_data[features]  # Features\n",
    "y = merged_data[target]  # Target\n",
    "\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the XGBoost model with class weights\n",
    "model = xgb.XGBClassifier(scale_pos_weight=10, random_state=42)  # Manually adjust scale_pos_weight\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "742424e4-b7c3-4290-b300-0a5fde5ec7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.3-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.3-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/124.9 MB 1.4 MB/s eta 0:01:27\n",
      "   ---------------------------------------- 0.5/124.9 MB 4.5 MB/s eta 0:00:28\n",
      "   ---------------------------------------- 1.2/124.9 MB 7.6 MB/s eta 0:00:17\n",
      "    --------------------------------------- 2.1/124.9 MB 10.4 MB/s eta 0:00:12\n",
      "    --------------------------------------- 3.1/124.9 MB 12.3 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 4.2/124.9 MB 14.3 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 5.1/124.9 MB 15.5 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 5.7/124.9 MB 15.2 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 6.3/124.9 MB 15.5 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 7.4/124.9 MB 15.8 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 8.5/124.9 MB 16.4 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 9.5/124.9 MB 17.0 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 10.7/124.9 MB 20.5 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 11.9/124.9 MB 21.8 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 13.1/124.9 MB 21.1 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 14.2/124.9 MB 21.8 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 15.1/124.9 MB 21.1 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 16.3/124.9 MB 23.4 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 17.6/124.9 MB 23.4 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 18.8/124.9 MB 23.4 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 20.1/124.9 MB 24.2 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 21.3/124.9 MB 24.3 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 22.8/124.9 MB 25.2 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 24.1/124.9 MB 26.2 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 25.3/124.9 MB 26.2 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 26.6/124.9 MB 27.3 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 27.8/124.9 MB 26.2 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 29.1/124.9 MB 27.3 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 30.4/124.9 MB 27.3 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 31.6/124.9 MB 27.3 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 33.1/124.9 MB 27.3 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 34.5/124.9 MB 27.3 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 35.9/124.9 MB 27.3 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 37.3/124.9 MB 28.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 38.6/124.9 MB 27.3 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 39.8/124.9 MB 28.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 39.8/124.9 MB 28.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 39.8/124.9 MB 28.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 39.8/124.9 MB 28.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 39.8/124.9 MB 28.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 39.8/124.9 MB 28.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 39.8/124.9 MB 28.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 39.8/124.9 MB 28.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 39.8/124.9 MB 28.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 39.8/124.9 MB 28.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 39.8/124.9 MB 28.5 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 40.6/124.9 MB 11.9 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 41.8/124.9 MB 12.1 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 43.0/124.9 MB 11.9 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 44.3/124.9 MB 11.9 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 45.3/124.9 MB 11.7 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 46.5/124.9 MB 11.7 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 47.5/124.9 MB 11.5 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 48.8/124.9 MB 11.5 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 49.8/124.9 MB 11.7 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 51.1/124.9 MB 25.2 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 52.4/124.9 MB 25.1 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 53.7/124.9 MB 24.2 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 55.0/124.9 MB 25.2 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 55.9/124.9 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 57.2/124.9 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 58.5/124.9 MB 27.3 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 59.8/124.9 MB 27.3 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 61.0/124.9 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 62.0/124.9 MB 26.2 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 63.3/124.9 MB 26.2 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 64.5/124.9 MB 27.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 65.6/124.9 MB 27.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 67.0/124.9 MB 27.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 68.3/124.9 MB 26.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 69.6/124.9 MB 26.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 71.1/124.9 MB 27.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 72.2/124.9 MB 26.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 73.5/124.9 MB 27.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 74.5/124.9 MB 27.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 75.9/124.9 MB 28.5 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 77.2/124.9 MB 28.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 78.8/124.9 MB 28.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 80.3/124.9 MB 29.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 81.6/124.9 MB 27.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 82.7/124.9 MB 28.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 84.2/124.9 MB 28.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 85.6/124.9 MB 28.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 86.0/124.9 MB 29.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 86.0/124.9 MB 29.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 86.0/124.9 MB 29.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 86.0/124.9 MB 29.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 86.0/124.9 MB 29.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 86.0/124.9 MB 29.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 86.0/124.9 MB 15.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 87.4/124.9 MB 15.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 88.7/124.9 MB 15.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 90.1/124.9 MB 15.6 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 91.6/124.9 MB 15.6 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 93.1/124.9 MB 15.6 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 94.6/124.9 MB 15.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 96.0/124.9 MB 15.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 97.0/124.9 MB 28.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 98.0/124.9 MB 28.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 99.4/124.9 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 100.5/124.9 MB 28.5 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 100.7/124.9 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 100.7/124.9 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 100.7/124.9 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 100.7/124.9 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 100.7/124.9 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 100.7/124.9 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 100.7/124.9 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 100.7/124.9 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 100.7/124.9 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 100.7/124.9 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 101.8/124.9 MB 11.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 103.4/124.9 MB 12.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 104.9/124.9 MB 12.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 106.3/124.9 MB 12.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 107.8/124.9 MB 12.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 109.3/124.9 MB 11.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 110.4/124.9 MB 12.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 111.9/124.9 MB 29.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 113.1/124.9 MB 31.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 114.6/124.9 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 116.1/124.9 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 117.6/124.9 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 119.1/124.9 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 120.6/124.9 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  121.9/124.9 MB 28.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  123.4/124.9 MB 29.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------- 124.9/124.9 MB 13.1 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bba6b486-cddf-47ae-a2df-0f61d0d539b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned datasets\n",
    "transactions_data_path = r\"C:\\Users\\DELL\\Downloads\\transactions_data.csv\"\n",
    "customers_data_path = r\"C:\\Users\\DELL\\Downloads\\customers_data.csv\"\n",
    "products_data_path = r\"C:\\Users\\DELL\\Downloads\\products_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33086ecf-de0d-49ef-810d-90bd4c9db4af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transactions_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(transactions_data, customers_data, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompany_ID\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(merged_data, products_data, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProduct_ID\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transactions_data' is not defined"
     ]
    }
   ],
   "source": [
    "merged_data = pd.merge(transactions_data, customers_data, on='Company_ID', how='left')\n",
    "merged_data = pd.merge(merged_data, products_data, on='Product_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9373c1d9-5557-4fee-97d6-205b2aa2819c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Transaction_ID  Company_ID  Product_ID  Quantity  \\\n",
      "0         0.0             1.0        88.0         6.0       NaN   \n",
      "1         1.0             2.0        29.0        19.0      16.0   \n",
      "2         2.0             NaN        28.0        18.0       6.0   \n",
      "3         3.0             4.0        85.0        12.0      12.0   \n",
      "4         4.0             5.0        47.0         3.0       8.0   \n",
      "\n",
      "  Transaction_Date  Product_Price_x  Total_Cost           Company_Name  \\\n",
      "0       2024/03/26    194379.147964   1075200.0    Elite Consulting 88   \n",
      "1    July 09, 2024     97930.993380   1428000.0    Sky  Industries  29   \n",
      "2       04/13/2024    126095.547778    940800.0                    NaN   \n",
      "3       09-06-2023              NaN   1008000.0      Green Ventures 85   \n",
      "4       07/06/2021     99575.609634    705600.0  Green  Industries  47   \n",
      "\n",
      "   Company_Profit                                           Address  \\\n",
      "0         75950.0           EDSA, Barangay 456, Taguig, Philippines   \n",
      "1         61952.0             Edsa, brgy. 606, makati, philippines!   \n",
      "2             NaN                                               NaN   \n",
      "3        113470.0        EDSA, Barangay 707, Cebu City, Philippines   \n",
      "4         31130.0  Taft Ave, Barangay 707, Mandaluyong, Philippines   \n",
      "\n",
      "             Product_Name Product_Price_y  \n",
      "0    RevenueVue Dashboard        ?179,200  \n",
      "1        EcoNomix Modeler         ?95,200  \n",
      "2  DashSync Analytics Hub        ?134,400  \n",
      "3        BudgetMaster Pro         ?84,000  \n",
      "4    TrendWise Forecaster        ?100,800  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned datasets from the specified paths\n",
    "transactions_data = pd.read_csv(r\"C:\\Users\\DELL\\Downloads\\transactions_data.csv\")\n",
    "customers_data = pd.read_csv(r\"C:\\Users\\DELL\\Downloads\\customers_data.csv\")\n",
    "products_data = pd.read_csv(r\"C:\\Users\\DELL\\Downloads\\products_data.csv\")\n",
    "\n",
    "# Merge the datasets\n",
    "merged_data = pd.merge(transactions_data, customers_data, on='Company_ID', how='left')\n",
    "merged_data = pd.merge(merged_data, products_data, on='Product_ID', how='left')\n",
    "\n",
    "# Check the merged data\n",
    "print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38ddf35c-eb07-4059-8692-661994182cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Transaction_ID  Company_ID  Product_ID  Quantity Transaction_Date  \\\n",
      "0             1.0        88.0         6.0       NaN       2024-03-26   \n",
      "1             2.0        29.0        19.0      16.0              NaT   \n",
      "2             NaN        28.0        18.0       6.0              NaT   \n",
      "3             4.0        85.0        12.0      12.0              NaT   \n",
      "4             5.0        47.0         3.0       8.0              NaT   \n",
      "\n",
      "   Product_Price_x  Total_Cost           Company_Name  Company_Profit  \\\n",
      "0    194379.147964   1075200.0    Elite Consulting 88         75950.0   \n",
      "1     97930.993380   1428000.0    Sky  Industries  29         61952.0   \n",
      "2    126095.547778    940800.0                Unknown             0.0   \n",
      "3              NaN   1008000.0      Green Ventures 85        113470.0   \n",
      "4     99575.609634    705600.0  Green  Industries  47         31130.0   \n",
      "\n",
      "                                            Address            Product_Name  \\\n",
      "0           EDSA, Barangay 456, Taguig, Philippines    RevenueVue Dashboard   \n",
      "1             Edsa, brgy. 606, makati, philippines!        EcoNomix Modeler   \n",
      "2                                               NaN  DashSync Analytics Hub   \n",
      "3        EDSA, Barangay 707, Cebu City, Philippines        BudgetMaster Pro   \n",
      "4  Taft Ave, Barangay 707, Mandaluyong, Philippines    TrendWise Forecaster   \n",
      "\n",
      "  Product_Price_y  Product_Price  \n",
      "0        ?179,200       179200.0  \n",
      "1         ?95,200        95200.0  \n",
      "2        ?134,400       134400.0  \n",
      "3         ?84,000        84000.0  \n",
      "4        ?100,800       100800.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "transactions_data = pd.read_csv(r\"C:\\Users\\DELL\\Downloads\\transactions_data.csv\")\n",
    "customers_data = pd.read_csv(r\"C:\\Users\\DELL\\Downloads\\customers_data.csv\")\n",
    "products_data = pd.read_csv(r\"C:\\Users\\DELL\\Downloads\\products_data.csv\")\n",
    "\n",
    "# Merge the datasets\n",
    "merged_data = pd.merge(transactions_data, customers_data, on='Company_ID', how='left')\n",
    "merged_data = pd.merge(merged_data, products_data, on='Product_ID', how='left')\n",
    "\n",
    "# Drop the 'Unnamed: 0' column\n",
    "merged_data = merged_data.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# 1. Standardize 'Transaction_Date' to datetime format\n",
    "merged_data['Transaction_Date'] = pd.to_datetime(merged_data['Transaction_Date'], errors='coerce')\n",
    "\n",
    "# 2. Clean 'Product_Price' by removing currency symbols and commas\n",
    "# Clean 'Product_Price' by removing currency symbols and commas\n",
    "merged_data['Product_Price'] = merged_data['Product_Price_y'].replace({r'\\?': '', r',': ''}, regex=True)\n",
    "merged_data['Product_Price'] = pd.to_numeric(merged_data['Product_Price'], errors='coerce')\n",
    "\n",
    "# 3. Handle missing values\n",
    "merged_data['Company_Name'] = merged_data['Company_Name'].fillna('Unknown')\n",
    "merged_data['Company_Profit'] = merged_data['Company_Profit'].fillna(0)\n",
    "\n",
    "# Check the cleaned data\n",
    "print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb7ea166-b63f-426c-8a00-9118d7a3b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the '?' and ',' characters from the 'Product_Price_y' column\n",
    "merged_data['Product_Price'] = merged_data['Product_Price_y'].replace({r'\\?': '', r',': ''}, regex=True)\n",
    "\n",
    "# Convert 'Product_Price' to numeric\n",
    "merged_data['Product_Price'] = pd.to_numeric(merged_data['Product_Price'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1c19c61-b6ae-4f5d-842c-0ff381cfb63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction_ID       2090\n",
      "Company_ID          10930\n",
      "Product_ID           3674\n",
      "Quantity             2102\n",
      "Transaction_Date    15495\n",
      "Product_Price_x      2086\n",
      "Total_Cost           1942\n",
      "Company_Name            0\n",
      "Company_Profit          0\n",
      "Address               970\n",
      "Product_Name         1641\n",
      "Product_Price_y      1641\n",
      "Product_Price        1641\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in merged_data\n",
    "print(merged_data.isnull().sum())\n",
    "\n",
    "# Drop rows where critical information is missing, or impute if necessary\n",
    "merged_data = merged_data.dropna(subset=['Product_Price', 'Quantity', 'Transaction_Date', 'Company_ID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "475b03b1-f0c5-4c0a-95b2-9c5c71bf748e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction_ID      192\n",
      "Company_ID            0\n",
      "Product_ID          448\n",
      "Quantity              0\n",
      "Transaction_Date      0\n",
      "Product_Price_x     228\n",
      "Total_Cost          228\n",
      "Company_Name          0\n",
      "Company_Profit        0\n",
      "Address             204\n",
      "Product_Name          0\n",
      "Product_Price_y       0\n",
      "Product_Price         0\n",
      "dtype: int64\n",
      "Transaction_ID      192\n",
      "Company_ID            0\n",
      "Product_ID          448\n",
      "Quantity              0\n",
      "Transaction_Date      0\n",
      "Product_Price_x     228\n",
      "Total_Cost          228\n",
      "Company_Name          0\n",
      "Company_Profit        0\n",
      "Address             204\n",
      "Product_Name          0\n",
      "Product_Price_y       0\n",
      "Product_Price         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(merged_data.isnull().sum())\n",
    "\n",
    "# Drop rows where critical information is missing\n",
    "merged_data = merged_data.dropna(subset=['Product_Price', 'Quantity', 'Transaction_Date', 'Company_ID'])\n",
    "\n",
    "# Verify the missing values after dropping\n",
    "print(merged_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "934628df-5d14-4450-9227-640bfd3f690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction_ID        0\n",
      "Company_ID            0\n",
      "Product_ID            0\n",
      "Quantity              0\n",
      "Transaction_Date      0\n",
      "Product_Price_x       0\n",
      "Total_Cost            0\n",
      "Company_Name          0\n",
      "Company_Profit        0\n",
      "Address             118\n",
      "Product_Name          0\n",
      "Product_Price_y       0\n",
      "Product_Price         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where critical information is missing\n",
    "merged_data = merged_data.dropna(subset=['Transaction_ID', 'Product_ID', 'Quantity', 'Product_Price_x', 'Total_Cost'])\n",
    "\n",
    "# Handle 'Transaction_Date' - either drop or fill with placeholder\n",
    "merged_data['Transaction_Date'] = merged_data['Transaction_Date'].fillna('Unknown')\n",
    "\n",
    "# Optionally, impute missing 'Product_Price_x' and 'Total_Cost' with median values\n",
    "merged_data['Product_Price_x'] = merged_data['Product_Price_x'].fillna(merged_data['Product_Price_x'].median())\n",
    "merged_data['Total_Cost'] = merged_data['Total_Cost'].fillna(merged_data['Total_Cost'].median())\n",
    "\n",
    "# Verify the missing values again\n",
    "print(merged_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc3df297-8704-4fd0-ac30-ab2f3aa7a470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction_ID      0\n",
      "Company_ID          0\n",
      "Product_ID          0\n",
      "Quantity            0\n",
      "Transaction_Date    0\n",
      "Product_Price_x     0\n",
      "Total_Cost          0\n",
      "Company_Name        0\n",
      "Company_Profit      0\n",
      "Address             0\n",
      "Product_Name        0\n",
      "Product_Price_y     0\n",
      "Product_Price       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values in the 'Address' column by filling with 'Unknown'\n",
    "merged_data['Address'] = merged_data['Address'].fillna('Unknown')\n",
    "\n",
    "# Verify the missing values again\n",
    "print(merged_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5eb1c066-ba2c-44f9-8ae7-86723e3df79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Company_ID  Recency  Purchase_Frequency  Total_Spending\n",
      "0        80.0     1204                  19      25508000.0\n",
      "1        70.0     1274                  11      20294400.0\n",
      "2        67.0       84                  16      17612000.0\n",
      "3        20.0      159                  14      19415200.0\n",
      "4        91.0      728                  12      18317600.0\n"
     ]
    }
   ],
   "source": [
    "# Recency: Days since last transaction\n",
    "merged_data['Recency'] = (merged_data['Transaction_Date'].max() - merged_data['Transaction_Date']).dt.days\n",
    "\n",
    "# Purchase Frequency (Number of transactions per customer)\n",
    "purchase_frequency = merged_data.groupby('Company_ID')['Transaction_ID'].count().reset_index()\n",
    "purchase_frequency.columns = ['Company_ID', 'Purchase_Frequency']\n",
    "merged_data = merged_data.merge(purchase_frequency, on='Company_ID', how='left')\n",
    "\n",
    "# Total Spending per customer\n",
    "total_spending = merged_data.groupby('Company_ID')['Total_Cost'].sum().reset_index()\n",
    "total_spending.columns = ['Company_ID', 'Total_Spending']\n",
    "merged_data = merged_data.merge(total_spending, on='Company_ID', how='left')\n",
    "\n",
    "# Check the updated data\n",
    "print(merged_data[['Company_ID', 'Recency', 'Purchase_Frequency', 'Total_Spending']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12781f32-e986-4c5c-836e-260f60396bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Transaction_ID  Company_ID  Product_ID  Quantity Transaction_Date  \\\n",
      "0             6.0        80.0        11.0       4.0       2021-07-12   \n",
      "1            24.0        70.0         1.0      19.0       2021-05-03   \n",
      "2            38.0        67.0         2.0      13.0       2024-08-05   \n",
      "3            39.0        20.0         1.0      12.0       2024-05-22   \n",
      "4            43.0        91.0         6.0      13.0       2022-10-31   \n",
      "\n",
      "   Product_Price_x  Total_Cost  Company_Profit Product_Price_y  Product_Price  \\\n",
      "0    160658.675350    627200.0        111227.0        ?156,800       156800.0   \n",
      "1    145162.097359   2660000.0         62252.0        ?140,000       140000.0   \n",
      "2    184039.022648   2016000.0         74653.0        ?168,000       168000.0   \n",
      "3    148830.094862   1820000.0        112720.0        ?140,000       140000.0   \n",
      "4    187921.970756   2150400.0        117123.0        ?179,200       179200.0   \n",
      "\n",
      "   ...  Address_Slex, barangay 123, pasig, philippines  \\\n",
      "0  ...                                           False   \n",
      "1  ...                                           False   \n",
      "2  ...                                           False   \n",
      "3  ...                                           False   \n",
      "4  ...                                           False   \n",
      "\n",
      "   Address_Slex, barangay 404, taguig, philippines  \\\n",
      "0                                            False   \n",
      "1                                            False   \n",
      "2                                            False   \n",
      "3                                            False   \n",
      "4                                            False   \n",
      "\n",
      "   Address_Slex, barangay 505, manila, philippines  \\\n",
      "0                                            False   \n",
      "1                                            False   \n",
      "2                                            False   \n",
      "3                                            False   \n",
      "4                                            False   \n",
      "\n",
      "   Address_Taft Ave, Barangay 123, Manila, Philippines!  \\\n",
      "0                                              False      \n",
      "1                                              False      \n",
      "2                                              False      \n",
      "3                                              False      \n",
      "4                                              False      \n",
      "\n",
      "   Address_Taft Ave, Barangay 456, Cebu City, Philippines  \\\n",
      "0                                              False        \n",
      "1                                              False        \n",
      "2                                              False        \n",
      "3                                              False        \n",
      "4                                              False        \n",
      "\n",
      "   Address_Taft Ave, Barangay 606, Makati, Philippines!  \\\n",
      "0                                              False      \n",
      "1                                              False      \n",
      "2                                              False      \n",
      "3                                              False      \n",
      "4                                              False      \n",
      "\n",
      "   Address_Taft Ave, Barangay 707, Mandaluyong, Philippines  \\\n",
      "0                                              False          \n",
      "1                                              False          \n",
      "2                                              False          \n",
      "3                                              False          \n",
      "4                                              False          \n",
      "\n",
      "   Address_Taft ave, barangay 202, baguio, philippines  \\\n",
      "0                                              False     \n",
      "1                                              False     \n",
      "2                                              False     \n",
      "3                                              False     \n",
      "4                                              False     \n",
      "\n",
      "   Address_Taft ave, brgy. 456, manila, philippines  Address_Unknown  \n",
      "0                                             False            False  \n",
      "1                                             False            False  \n",
      "2                                             False            False  \n",
      "3                                             False            False  \n",
      "4                                             False            False  \n",
      "\n",
      "[5 rows x 207 columns]\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoding for categorical variables\n",
    "merged_data = pd.get_dummies(merged_data, columns=['Product_Name', 'Company_Name', 'Address'], drop_first=True)\n",
    "\n",
    "# Check the first few rows after encoding\n",
    "print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9b7e4df-b560-426f-a6cb-4acb395ae84f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Purchase_Likelihood'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Purchase_Likelihood'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPurchase_Likelihood\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m merged_data[features]\n\u001b[1;32m----> 6\u001b[0m y \u001b[38;5;241m=\u001b[39m merged_data[target]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Split the data into training and test sets\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Purchase_Likelihood'"
     ]
    }
   ],
   "source": [
    "# Assuming 'Purchase_Likelihood' is already in the merged_data and is binary (0 or 1)\n",
    "features = ['Recency', 'Purchase_Frequency', 'Total_Spending'] + [col for col in merged_data.columns if col.startswith('Product_Name') or col.startswith('Company_Name') or col.startswith('Address')]\n",
    "target = 'Purchase_Likelihood'\n",
    "\n",
    "X = merged_data[features]\n",
    "y = merged_data[target]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shapes of the data\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "150ec41b-b446-49c5-9c3e-353757c1d7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Company_ID  Recency  Purchase_Frequency  Total_Spending  \\\n",
      "0        80.0     1204                  19      25508000.0   \n",
      "1        70.0     1274                  11      20294400.0   \n",
      "2        67.0       84                  16      17612000.0   \n",
      "3        20.0      159                  14      19415200.0   \n",
      "4        91.0      728                  12      18317600.0   \n",
      "\n",
      "   Purchase_Likelihood  \n",
      "0                    0  \n",
      "1                    0  \n",
      "2                    0  \n",
      "3                    0  \n",
      "4                    0  \n"
     ]
    }
   ],
   "source": [
    "# Create the Purchase_Likelihood feature\n",
    "# Define thresholds for high/low likelihood (this is a basic approach, and you can fine-tune the thresholds)\n",
    "\n",
    "high_recency_threshold = merged_data['Recency'].median()\n",
    "high_frequency_threshold = merged_data['Purchase_Frequency'].median()\n",
    "high_spending_threshold = merged_data['Total_Spending'].median()\n",
    "\n",
    "# Define the Purchase_Likelihood based on these thresholds\n",
    "merged_data['Purchase_Likelihood'] = (\n",
    "    (merged_data['Recency'] <= high_recency_threshold) & \n",
    "    (merged_data['Purchase_Frequency'] >= high_frequency_threshold) & \n",
    "    (merged_data['Total_Spending'] >= high_spending_threshold)\n",
    ").astype(int)  # 1 for high likelihood, 0 for low likelihood\n",
    "\n",
    "# Check the first few rows of the dataset with the new target variable\n",
    "print(merged_data[['Company_ID', 'Recency', 'Purchase_Frequency', 'Total_Spending', 'Purchase_Likelihood']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0a0817a-617c-495d-ae95-5c238acd0ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(975, 3) (244, 3) (975,) (244,)\n"
     ]
    }
   ],
   "source": [
    "# Define the features (X) and the target (y)\n",
    "features = ['Recency', 'Purchase_Frequency', 'Total_Spending']  # You can add more features here if needed\n",
    "target = 'Purchase_Likelihood'\n",
    "\n",
    "X = merged_data[features]\n",
    "y = merged_data[target]\n",
    "\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shapes of the datasets\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "576ab0d0-6137-49bd-89ff-ac1b94677a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[191   0]\n",
      " [  0  53]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       191\n",
      "           1       1.00      1.00      1.00        53\n",
      "\n",
      "    accuracy                           1.00       244\n",
      "   macro avg       1.00      1.00      1.00       244\n",
      "weighted avg       1.00      1.00      1.00       244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RandomForest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cdf3fc-7918-4697-a714-bdb0b611836e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
